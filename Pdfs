import urllib.request

def download_pdfs(url):
  """Downloads PDFs from a website.

  Args:
    url: The URL of the website containing the PDFs.
  """
  # Send an HTTP request to retrieve the webpage content
  response = urllib.request.urlopen(url)

  # Read the webpage content
  html_content = response.read()

  # Parse the HTML content using regular expressions (assuming simpler HTML structure)
  import re
  pdf_links = re.findall(r'(?i)<a[^>]+href="([^"]+\.pdf)"', html_content.decode('utf-8'))

  # Iterate through each link and download the PDF
  for link in pdf_links:
    pdf_url = url + link if not link.startswith('http') else link  # Handle relative and absolute URLs
    download_pdf(pdf_url)

def download_pdf(pdf_url):
  """Downloads a PDF from a URL.

  Args:
    pdf_url: The URL of the PDF to download.
  """
  # Send an HTTP request to retrieve the PDF content
  response = urllib.request.urlopen(pdf_url)

  # Get the filename from the response headers (if available)
  filename = pdf_url.split('/')[-1]  # Extract filename from URL as a fallback
  content_disposition = response.headers.get('Content-Disposition')
  if content_disposition:
    filename = content_disposition.split('filename=')[-1].strip('"')

  # Save the downloaded PDF
  with open(filename, 'wb') as f:
    f.write(response.read())

  print(f"Downloaded PDF: {filename}")

# Example usage
website_url = "https://www.example.com/"  # Replace with the actual website URL
download_pdfs(website_url)
